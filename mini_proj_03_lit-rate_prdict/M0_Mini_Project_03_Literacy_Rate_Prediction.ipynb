{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RWR22OAlTBlU"
   },
   "source": [
    "# Advanced Certification in Computational Data Science\n",
    "## A Program by IISc and TalentSprint\n",
    "### Mini-Project (Ungraded)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DIfh2REWFr6p"
   },
   "source": [
    "## Learning Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LrIlNFZeKMsx"
   },
   "source": [
    "At the end of this experiment, you will be able to:\n",
    "\n",
    "* perform Data preprocessing\n",
    "* implement ML classification algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V3vgcWwOF2cK"
   },
   "source": [
    "## Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mfv616s92gl_"
   },
   "source": [
    "We will be using district wise demographics, enrollments, and teacher indicator data to predict whether the literacy rate is high/ medium/ low in each district."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "md2IjdMdGCWm"
   },
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3B5ztQVbKMsz"
   },
   "source": [
    "Data preprocessing is an important step in solving every machine learning problem. Most of\n",
    "the datasets used with Machine Learning problems need to be processed / cleaned / transformed\n",
    "so that a Machine Learning algorithm can be trained on it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QsxaJLZAKMs0"
   },
   "source": [
    "There are different steps involved in Data Preprocessing. These steps are as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QF3Eg-5pKMs1"
   },
   "source": [
    "    1. Data Cleaning → In this step the primary focus is on\n",
    "        - Handling missing data\n",
    "        - Handling noisy data\n",
    "        - Detection and removal of outliers\n",
    "    \n",
    "    2. Data Integration → This process is used when data is gathered from various data sources\n",
    "    and data are combined to form consistent data. This data after performing cleaning is used\n",
    "    for analysis.\n",
    "    \n",
    "    3. Data Transformation → In this step we will convert the raw data into a specified format according to the need of the model we are building. There are many options used for\n",
    "    transforming the data as below:\n",
    "        - Normalization\n",
    "        - Aggregation\n",
    "        - Generalization\n",
    "        \n",
    "    4. Data Reduction → Following data transformation and scaling, the redundancy within the data is removed and is organized efficiently.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "4UpjelxSBoGN"
   },
   "outputs": [],
   "source": [
    "# @title Download the datasets\n",
    "from IPython import get_ipython\n",
    "\n",
    "ipython = get_ipython()\n",
    "  \n",
    "notebook=\"U1_MH1_Data_Munging\" #name of the notebook\n",
    "\n",
    "def setup():\n",
    "    from IPython.display import HTML, display\n",
    "    ipython.magic(\"sx wget https://cdn.iisc.talentsprint.com/aiml/Experiment_related_data/B15_Data_Munging.zip\")\n",
    "    ipython.magic(\"sx unzip B15_Data_Munging.zip\")\n",
    "    print(\"Data downloaded successfully\")\n",
    "    return\n",
    "\n",
    "setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e7VD8dJgGhVw"
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hZSlj_nWKMs4"
   },
   "source": [
    "## Exercise 1 - Load and Explore the Data \n",
    "1. We have three different files\n",
    "\n",
    "  * Districtwise_Basicdata.csv\n",
    "  * Districtwise_Enrollment_details_indicator.csv\n",
    "  * Districtwise_Teacher_indicator.csv\n",
    "\n",
    "  These files contain the necessary data to solve the problem. <br>\n",
    "\n",
    "2. Load the files based on **team allocation** mentioned below. Observe the header level details, data records while loading the data.\n",
    "  \n",
    "  Hint : Use read_csv from pandas with [skiprows or header](https://towardsdatascience.com/import-csv-files-as-pandas-dataframe-with-skiprows-skipfooter-usecols-index-col-and-header-fbf67a2f92a) options.\n",
    "\n",
    "3. Read the columns of the dataset and rename if required.\n",
    "\n",
    "  Hint : Rename column names (if any) using the following [link](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.rename.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L7xw5qs68uWE"
   },
   "source": [
    "Team allocation for dataset selection\n",
    "\n",
    "    Team A = 1,3,5,7,9\n",
    "        Districtwise_Basicdata.csv\n",
    "        Districtwise_Enrollment_details_indicator.csv\n",
    "\n",
    "    Team B = 2,4,6,8,10\n",
    "        Districtwise_Basicdata.csv\n",
    "        Districtwise_Teacher_indicator.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_XRregSb9wdB"
   },
   "outputs": [],
   "source": [
    "# Importing all the required packages and add neccesary imports if required\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "la-GbwZKBD0L"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE for loading and exploring the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K2A3OKraKMs9"
   },
   "source": [
    "## Exercise 2  - Data Integration\n",
    "\n",
    "As the required data is present in different datasets, we need to **integrate both to make a single dataframe/dataset**.\n",
    "  * For integrating the datasets, create a unique identifier for each row in both the dataframes so that it can be used to map the data in different files.\n",
    "   \n",
    "    * Combine year, state code, district code columns and form a new unique identifier column, refer this [link](https://stackoverflow.com/questions/33098383/merge-multiple-column-values-into-one-column-in-python-pandas).\n",
    "    * Set the identifier column as the index for each dataframe.\n",
    "\n",
    "    * Integrate the dataframes using the above index\n",
    "     \n",
    "     Hint: For merging or joining the datasets, refer to this [link](https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html)\n",
    "\n",
    "**Example:** Data of the district Anantapur in Andrapradesh, which is present in different files should form a single row after integrating the datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wXvBjPIH4CWH"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE for integrating the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_jcX4aRsKMtE"
   },
   "source": [
    "## Exercise 3 - Data Cleaning \n",
    "\n",
    "1.  **overall_lit** is our target variable. Delete rows with missing overall_lit value\n",
    "\n",
    "   Hint: Refer to the link [dropna](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dropna.html).\n",
    "\n",
    "\n",
    "2.  Convert categorical values to numerical values.\n",
    "\n",
    "  For example, if a feature contains categorical values such as dog, cat, mouse, etc then replace them with 1, 2, 3, etc or use [Sklearn LabelEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html) \n",
    "\n",
    "3. Replace the missing values in any other column appropriately with mean / median / mode.\n",
    "\n",
    "  Hint: Use pandas [fillna](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.fillna.html) function to replace the missing values\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6IhKVYJ24xVn"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE for data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SzsY-knQKMtI"
   },
   "source": [
    "## Exercise 4 \n",
    "\n",
    "1. Remove the unneccesary columns which are not contributing to the overall literacy rate\n",
    "\n",
    "2. Verify if there are any duplicate columns and remove them.\n",
    "\n",
    "  For example: state name and district name are same as state code and district code.\n",
    "\n",
    "3. Make sure that the final dataframe has no null or nan values. Delete the rows with missing values.\n",
    "\n",
    "   Hint: Verify with df.isna() for nan values in the dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-hU2poUeKMtT"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE for cleaning the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cJo0t2403sf7"
   },
   "source": [
    "## Exercise 5 - Apply Correlation Matrix\n",
    "\n",
    "Correlation is a statistical technique that can show whether, and how strongly, pairs of variables are related. More number of features does not imply better accuracy. More features may lead to a decline in the accuracy and create noise in the model, if they contain any irrelevant features.\n",
    "\n",
    "*Features with high correlation value will imply the same meaning. Hence remove the highly correlated features*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KLRn2Zv-pwdR"
   },
   "source": [
    "**Function Description:**\n",
    "\n",
    "Create a function `remove_Highly_Correlated()` function, which removes highly correlated features in the dataframe.\n",
    "- Creates a correlation matrix of row and column wise features\n",
    "- Extracts only upper triangular matrix as correlation matrix, which will have the same values below and above the diagonal\n",
    "- Removes columns which are having correlation value more than the threshold value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kEN36Fj0o2TQ"
   },
   "outputs": [],
   "source": [
    "def remove_Highly_Correlated(df, bar=0.9):\n",
    "  # Creates correlation matrix\n",
    "  corr = df.corr()\n",
    "\n",
    "  # Set Up Mask To Hide Upper Triangle\n",
    "  mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "  tri_df = corr.mask(mask)\n",
    "\n",
    "  # Finding features with correlation value more than specified threshold value (bar=0.9)\n",
    "  highly_cor_col = [col for col in tri_df.columns if any(tri_df[col] > bar )]\n",
    "  print(\"length of highly correlated columns\",len(highly_cor_col))\n",
    "\n",
    "  # Drop the highly correlated columns\n",
    "  reduced_df = df.drop(highly_cor_col, axis = 1)\n",
    "  print(\"shape of total data\",df.shape,\"shape of reduced data\",reduced_df.shape)\n",
    "  return reduced_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p6VMZGXL5O45"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE to remove highly correlated features from the dataframe by calling above function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e_yZN3u9Vud7"
   },
   "source": [
    "## Exercise 6\n",
    "\n",
    "Perform Mean Correction and Standard Scaling on the data feature/column wise.\n",
    "\n",
    "**Hint:** In order to understand the idea behind the terms used above, you may refer the following link: \n",
    "\n",
    "[StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oAiQVqsPl1ZM"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xTB9sMRsVxyj"
   },
   "source": [
    "## Exercise 7\n",
    "\n",
    "Apply different classifiers on the preprocessed data and figure out which classifier gives the best result.\n",
    "\n",
    "* Split the data into train and test\n",
    "\n",
    "* Fit the model with train data and find its accuracy on test data\n",
    "\n",
    "### Expected Accuracy is above 90%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yRgkeFVBqv0W"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE for applying different classifiers"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "M0_Mini_Project_03_Literacy_Rate_Prediction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
